{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('poco_f1_edited.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_name</th>\n",
       "      <th>review_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Such a wonderful phone in this range\\n845 snap...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Expected a replacement and had battery issue b...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Received the phone was happy to see the packag...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Its all what i wanted from a phone.The fingerp...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Beast at this price range</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_name review_rating\n",
       "0  Such a wonderful phone in this range\\n845 snap...             5\n",
       "1  Expected a replacement and had battery issue b...             2\n",
       "2  Received the phone was happy to see the packag...             5\n",
       "3  Its all what i wanted from a phone.The fingerp...             5\n",
       "4                          Beast at this price range             5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(df['review_rating'])):\n",
    "    val = df['review_rating'][index]\n",
    "   # print(val)\n",
    "    try:\n",
    "        df['review_rating'][index] = int(val)\n",
    "        \n",
    "    except ValueError:\n",
    "        df['review_rating'][index] = 3\n",
    "        #print(val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = type(df['review_rating'][0])\n",
    "for index,row in df.iterrows():\n",
    "    #print(type(row['review_rating']))\n",
    "    if type(row['review_rating'])  != s:\n",
    "        df['review_rating'][index]= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "positive_list = []\n",
    "negative_list = []\n",
    "for i in range(11862):\n",
    "    if(df.review_rating[i]<3):\n",
    "        negative_list.append(df.review_name[i])\n",
    "    else:\n",
    "        positive_list.append(df.review_name[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(11862):\n",
    "    df.review_rating[i] =int(df.review_rating[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list =[]\n",
    "n_list=[]\n",
    "for i in range(len(positive_list)):\n",
    "    p_list.append(word_tokenize(positive_list[i]))\n",
    "for i in range(len(negative_list)):\n",
    "    n_list.append(word_tokenize(negative_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Such a wonderful phone in this range\n",
      "845 snapdragon which is the latest that is in it.\n",
      "Just go for it.\n",
      "Sexy phone\n"
     ]
    }
   ],
   "source": [
    "print(positive_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('phone', 7672), ('good', 6031), ('camera', 3825), ('...', 2986), ('best', 2796), ('battery', 2673), ('performance', 2601), ('..', 2190), ('price', 2174), ('use', 1772)]\n",
      "Accuracy is: 0.8932538050185109\n",
      "Most Informative Features\n",
      "                  reject = True           Negati : Positi =     36.2 : 1.0\n",
      "             motherboard = True           Negati : Positi =     33.1 : 1.0\n",
      "                   10-15 = True           Negati : Positi =     28.9 : 1.0\n",
      "               executive = True           Negati : Positi =     28.9 : 1.0\n",
      "                manually = True           Negati : Positi =     28.9 : 1.0\n",
      "                replaced = True           Negati : Positi =     28.9 : 1.0\n",
      "                   swell = True           Negati : Positi =     28.9 : 1.0\n",
      "               defective = True           Negati : Positi =     27.3 : 1.0\n",
      "                 compass = True           Negati : Positi =     23.6 : 1.0\n",
      "                  sudden = True           Negati : Positi =     23.6 : 1.0\n",
      "None\n",
      "this camera is bullshit Positive\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    positive_tweet_tokens = p_list\n",
    "    negative_tweet_tokens = n_list\n",
    "\n",
    "    positive_cleaned_tokens_list = []\n",
    "    negative_cleaned_tokens_list = []\n",
    "\n",
    "    for tokens in positive_tweet_tokens:\n",
    "        positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "    for tokens in negative_tweet_tokens:\n",
    "        negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "    all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "\n",
    "    freq_dist_pos = FreqDist(all_pos_words)\n",
    "    print(freq_dist_pos.most_common(10))\n",
    "\n",
    "    positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "    negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
    "\n",
    "    positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                         for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "    negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                         for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "    dataset = positive_dataset + negative_dataset\n",
    "\n",
    "    random.shuffle(dataset)\n",
    "\n",
    "    train_data = dataset[:7000]\n",
    "    test_data = dataset[7000:]\n",
    "\n",
    "    classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "    print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "    print(classifier.show_most_informative_features(10))\n",
    "\n",
    "    custom_tweet = \"this camera is bullshit\"\n",
    "\n",
    "    custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "\n",
    "    print(custom_tweet, classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wonderful', 'phone', 'range', '845', 'snapdragon', 'late', 'go', 'sexy', 'phone']\n"
     ]
    }
   ],
   "source": [
    "print(positive_cleaned_tokens_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "  'The food we had yesterday was delicious',\n",
    "  'My time in Italy was very enjoyable',\n",
    "  'I found the meal to be tasty',\n",
    "  'The internet was slow.',\n",
    "  'Our experience was suboptimal',\n",
    "  'the camera is not great'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The det food NOUN DET []\n",
      "food nsubj was AUX NOUN [The, had]\n",
      "we nsubj had VERB PRON []\n",
      "had relcl food NOUN VERB [we, yesterday]\n",
      "yesterday npadvmod had VERB NOUN []\n",
      "was ROOT was AUX AUX [food, delicious]\n",
      "delicious acomp was AUX ADJ []\n",
      "My poss time NOUN PRON []\n",
      "time nsubj was AUX NOUN [My, in]\n",
      "in prep time NOUN ADP [Italy]\n",
      "Italy pobj in ADP PROPN []\n",
      "was ROOT was AUX AUX [time, enjoyable]\n",
      "very advmod enjoyable ADJ ADV []\n",
      "enjoyable acomp was AUX ADJ [very]\n",
      "I nsubj found VERB PRON []\n",
      "found ROOT found VERB VERB [I, be]\n",
      "the det meal NOUN DET []\n",
      "meal nsubj be VERB NOUN [the]\n",
      "to aux be VERB PART []\n",
      "be ccomp found VERB VERB [meal, to, tasty]\n",
      "tasty acomp be VERB ADJ []\n",
      "The det internet NOUN DET []\n",
      "internet nsubj was AUX NOUN [The]\n",
      "was ROOT was AUX AUX [internet, slow, .]\n",
      "slow acomp was AUX ADJ []\n",
      ". punct was AUX PUNCT []\n",
      "Our poss experience NOUN PRON []\n",
      "experience nsubj was AUX NOUN [Our]\n",
      "was ROOT was AUX AUX [experience, suboptimal]\n",
      "suboptimal acomp was AUX ADJ []\n",
      "the det camera NOUN DET []\n",
      "camera nsubj is AUX NOUN [the]\n",
      "is ROOT is AUX AUX [camera, not, great]\n",
      "not neg is AUX PART []\n",
      "great acomp is AUX ADJ []\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "              token.pos_,[child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the camera is not great\n",
      "great\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    doc = nlp(sentence)\n",
    "    descriptive_term = ''\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADJ':\n",
    "            prepend = ''\n",
    "            for child in token.children:\n",
    "                if child.pos_ != 'ADV':\n",
    "                    continue\n",
    "                prepend += child.text + ' '\n",
    "            descriptive_term = prepend + token.text\n",
    "print(sentence)\n",
    "print(descriptive_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'aspect': 'food', 'description': ''}, {'aspect': 'time', 'description': 'very enjoyable'}, {'aspect': 'meal', 'description': ''}, {'aspect': 'internet', 'description': ''}, {'aspect': 'experience', 'description': ''}, {'aspect': 'camera', 'description': ''}]\n"
     ]
    }
   ],
   "source": [
    "aspects = []\n",
    "for sentence in sentences:\n",
    "    doc = nlp(sentence)\n",
    "    descriptive_term = ''\n",
    "    target = ''\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'nsubj' and token.pos_ == 'NOUN':\n",
    "            target = token.text\n",
    "        if token.pos_ == 'ADJ':\n",
    "            prepend = ''\n",
    "            for child in token.children:\n",
    "                if child.pos_ != 'ADV':\n",
    "                    continue\n",
    "                prepend += child.text + ' '\n",
    "                descriptive_term = prepend + token.text\n",
    "    aspects.append({'aspect': target,\n",
    "    'description': descriptive_term})\n",
    "print(aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'aspect': 'food', 'description': '', 'sentiment': Sentiment(polarity=0.0, subjectivity=0.0)}, {'aspect': 'time', 'description': 'very enjoyable', 'sentiment': Sentiment(polarity=0.65, subjectivity=0.78)}, {'aspect': 'meal', 'description': '', 'sentiment': Sentiment(polarity=0.0, subjectivity=0.0)}, {'aspect': 'internet', 'description': '', 'sentiment': Sentiment(polarity=0.0, subjectivity=0.0)}, {'aspect': 'experience', 'description': '', 'sentiment': Sentiment(polarity=0.0, subjectivity=0.0)}, {'aspect': 'camera', 'description': '', 'sentiment': Sentiment(polarity=0.0, subjectivity=0.0)}]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "for aspect in aspects:\n",
    "    aspect['sentiment'] = TextBlob(aspect['description']).sentiment\n",
    "print(aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delicious food.\n",
      "positive\n",
      "Very Slow internet.\n",
      "negative\n",
      "Suboptimal experience.\n",
      "negative\n",
      "Enjoyable food.\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "# We train the NaivesBayesClassifier\n",
    "train = [\n",
    "  ('Slow internet.', 'negative'),\n",
    "  ('Delicious food', 'positive'),\n",
    "  ('Suboptimal experience', 'negative'),\n",
    "  ('Very enjoyable time', 'positive'),\n",
    "  ('delicious food.', 'neg')\n",
    "]\n",
    "cl = NaiveBayesClassifier(train)\n",
    "# And then we try to classify some sample sentences.\n",
    "blob = TextBlob(\"Delicious food. Very Slow internet. Suboptimal experience. Enjoyable food.\", classifier=cl)\n",
    "for s in blob.sentences:\n",
    "    print(s)\n",
    "    print(s.classify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_pos = []\n",
    "for line in positive_list:\n",
    "    cleaned_data_pos.append(remove_noise(tokens,stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_neg = []\n",
    "for line in negative_list:\n",
    "    cleaned_data_neg.append(remove_noise(tokens,stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_pos = []\n",
    "for line in cleaned_data_pos:\n",
    "    sentence_pos.append(TreebankWordDetokenizer().detokenize(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local gui'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_neg = []\n",
    "for line in cleaned_data_neg:\n",
    "    sentence_neg.append(TreebankWordDetokenizer().detokenize(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local gui'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_neg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list = []\n",
    "n_list=[]\n",
    "for i in range(len(positive_list)):\n",
    "    p_list.append(word_tokenize(positive_list[i]))\n",
    "for i in range(len(negative_list)):\n",
    "    n_list.append(word_tokenize(negative_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_token = p_list\n",
    "positive_cleaned =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokens in positive_token:\n",
    "    positive_cleaned.append(remove_noise(tokens,stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pos_words = get_all_words(positive_cleaned)\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wonderful',\n",
       " 'phone',\n",
       " 'range',\n",
       " '845',\n",
       " 'snapdragon',\n",
       " 'late',\n",
       " 'go',\n",
       " 'sexy',\n",
       " 'phone']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_cleaned[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(positive_cleaned)):\n",
    "    positive_cleaned[i]=TreebankWordDetokenizer().detokenize(positive_cleaned[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_token = n_list\n",
    "negative_cleaned=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokens in negative_token:\n",
    "    negative_cleaned.append(remove_noise(tokens, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(negative_cleaned)):\n",
    "    negative_cleaned[i]=TreebankWordDetokenizer().detokenize(negative_cleaned[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_list = positive_cleaned + negative_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = review_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wonderful amod range NOUN ADJ []\n",
      "phone compound range NOUN NOUN []\n",
      "range nsubj go VERB NOUN [wonderful, phone, snapdragon]\n",
      "845 nummod snapdragon NOUN NUM []\n",
      "snapdragon appos range NOUN NOUN [845]\n",
      "late advmod go VERB ADV []\n",
      "go ROOT go VERB VERB [range, late, phone]\n",
      "sexy amod phone NOUN ADJ []\n",
      "phone dobj go VERB NOUN [sexy]\n",
      "received ROOT received VERB VERB [happy, see, 've, need, perfectly]\n",
      "phone compound happy ADJ NOUN []\n",
      "happy dobj received VERB ADJ [phone]\n",
      "see advcl received VERB VERB [guard, back, cover]\n",
      "packaging nsubj get NOUN NOUN []\n",
      "get compound guard NOUN NOUN [packaging]\n",
      "screen compound guard NOUN NOUN []\n",
      "guard dobj see VERB NOUN [get, screen]\n",
      "back advmod see VERB ADV []\n",
      "cover conj see VERB NOUN [guard]\n",
      "early amod guard NOUN ADJ []\n",
      "get compound screen NOUN NOUN []\n",
      "screen compound guard NOUN NOUN [get]\n",
      "guard dobj cover NOUN NOUN [early, screen, layern't, perfect]\n",
      "early amod scratch NOUN ADJ []\n",
      "anti amod scratch NOUN ADJ []\n",
      "scratch compound layern't PROPN NOUN [early, anti]\n",
      "layern't appos guard NOUN PROPN [scratch]\n",
      "last amod perfect ADJ ADJ []\n",
      "long amod perfect ADJ ADJ []\n",
      "3rd amod generation NOUN ADJ []\n",
      "generation compound phone NOUN NOUN [3rd]\n",
      "gorilla compound phone NOUN NOUN []\n",
      "glass compound phone NOUN NOUN []\n",
      "phone compound perfect ADJ NOUN [generation, gorilla, glass]\n",
      "perfect amod guard NOUN ADJ [last, long, phone]\n",
      "although mark 've VERB SCONJ []\n",
      "build compound quality NOUN NOUN []\n",
      "quality nsubj 've VERB NOUN [build]\n",
      "could aux 've VERB AUX []\n",
      "'ve advcl received VERB VERB [although, quality, could, cater]\n",
      "good amod cater NOUN ADJ [happy]\n",
      "happy advmod good ADJ ADJ []\n",
      "one nummod cater NOUN NUM []\n",
      "cater dobj 've VERB NOUN [good, one]\n",
      "need conj received VERB VERB []\n",
      "perfectly advmod received VERB ADV []\n",
      "want ROOT want VERB VERB [unlock, phone]\n",
      "phone.the det scanner NOUN DET []\n",
      "fingerprint compound scanner NOUN NOUN []\n",
      "scanner nsubj unlock VERB NOUN [phone.the, fingerprint]\n",
      "fast.the dep unlock VERB PROPN []\n",
      "ir amod face NOUN ADJ []\n",
      "face compound unlock VERB NOUN [ir]\n",
      "unlock ccomp want VERB VERB [scanner, fast.the, face, performance]\n",
      "fast.and compound performance NOUN X []\n",
      "performance dobj unlock VERB NOUN [fast.and]\n",
      "also advmod great.overall DET ADV []\n",
      "great.overall amod phone NOUN DET [also]\n",
      "great amod phone NOUN ADJ []\n",
      "phone npadvmod want VERB NOUN [great.overall, great]\n",
      "beast compound price NOUN NOUN []\n",
      "price compound range NOUN NOUN [beast]\n",
      "range ROOT range NOUN NOUN [price]\n",
      "amazing amod pic NOUN ADJ []\n",
      "pic nsubj love NOUN NOUN [amazing, n, video]\n",
      "n cc pic NOUN CCONJ []\n",
      "video conj pic NOUN NOUN []\n",
      "... punct love NOUN PUNCT []\n",
      "love compound phone NOUN NOUN [pic, ...]\n",
      "phone ROOT phone NOUN NOUN [love, .., nice, life, .., phone]\n",
      ".. punct phone NOUN PUNCT []\n",
      "nice amod phone NOUN ADJ [n, fast]\n",
      "n cc nice ADJ CCONJ []\n",
      "fast conj nice ADJ ADJ []\n",
      ".. punct life NOUN PUNCT []\n",
      "awesome amod life NOUN ADJ []\n",
      "pic nmod life NOUN NOUN []\n",
      "great amod life NOUN ADJ []\n",
      "battery compound life NOUN NOUN []\n",
      "life appos phone NOUN NOUN [.., awesome, pic, great, battery]\n",
      ".. punct phone NOUN PUNCT []\n",
      "real amod flagship NOUN ADJ []\n",
      "flagship compound phone NOUN NOUN [real]\n",
      "phone appos phone NOUN NOUN [flagship]\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "  doc = nlp(sentence)\n",
    "  for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "      token.pos_,[child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wonderful phone range 845 snapdragon late go sexy phone\n",
      "sexy\n",
      "received phone happy see packaging get screen guard back cover early get screen guard early anti scratch layern't last long 3rd generation gorilla glass phone perfect although build quality could've good happy one cater need perfectly\n",
      "happy\n",
      "want phone.the fingerprint scanner fast.the ir face unlock fast.and performance also great.overall great phone\n",
      "great\n",
      "beast price range\n",
      "\n",
      "amazing pic n video...love phone .. nice n fast .. awesome pic great battery life .. real flagship phone\n",
      "real\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "  doc = nlp(sentence)\n",
    "  descriptive_term = ''\n",
    "  for token in doc:\n",
    "    if token.pos_ == 'ADJ':\n",
    "      descriptive_term = token\n",
    "  print(sentence)\n",
    "  print(descriptive_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wonderful phone range 845 snapdragon late go sexy phone\n",
      "sexy\n",
      "received phone happy see packaging get screen guard back cover early get screen guard early anti scratch layern't last long 3rd generation gorilla glass phone perfect although build quality could've good happy one cater need perfectly\n",
      "happy\n",
      "want phone.the fingerprint scanner fast.the ir face unlock fast.and performance also great.overall great phone\n",
      "great\n",
      "beast price range\n",
      "\n",
      "amazing pic n video...love phone .. nice n fast .. awesome pic great battery life .. real flagship phone\n",
      "real\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "  doc = nlp(sentence)\n",
    "  descriptive_term = ''\n",
    "  for token in doc:\n",
    "    if token.pos_ == 'ADJ':\n",
    "      prepend = ''\n",
    "      for child in token.children:\n",
    "        if child.pos_ != 'ADV':\n",
    "          continue\n",
    "        prepend += child.text + ' '\n",
    "      descriptive_term = prepend + token.text\n",
    "  print(sentence)\n",
    "  print(descriptive_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'aspect': 'range', 'description': 'sexy'}, {'aspect': 'quality', 'description': 'happy'}, {'aspect': 'scanner', 'description': 'great'}, {'aspect': '', 'description': ''}, {'aspect': 'pic', 'description': 'real'}]\n"
     ]
    }
   ],
   "source": [
    "aspects = []\n",
    "for sentence in sentences:\n",
    "  doc = nlp(sentence)\n",
    "  descriptive_term = ''\n",
    "  target = ''\n",
    "  for token in doc:\n",
    "    if token.dep_ == 'nsubj' and token.pos_ == 'NOUN':\n",
    "      target = token.text\n",
    "    if token.pos_ == 'ADJ':\n",
    "      prepend = ''\n",
    "      for child in token.children:\n",
    "        if child.pos_ != 'ADV':\n",
    "          continue\n",
    "        prepend += child.text + ' '\n",
    "      descriptive_term = prepend + token.text\n",
    "  aspects.append({'aspect': target,\n",
    "    'description': descriptive_term})\n",
    "print(aspects)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'aspect': 'range', 'description': 'sexy', 'sentiment': Sentiment(polarity=0.5, subjectivity=1.0)}, {'aspect': 'quality', 'description': 'happy', 'sentiment': Sentiment(polarity=0.8, subjectivity=1.0)}, {'aspect': 'scanner', 'description': 'great', 'sentiment': Sentiment(polarity=0.8, subjectivity=0.75)}, {'aspect': '', 'description': '', 'sentiment': Sentiment(polarity=0.0, subjectivity=0.0)}, {'aspect': 'pic', 'description': 'real', 'sentiment': Sentiment(polarity=0.2, subjectivity=0.30000000000000004)}]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "for aspect in aspects:\n",
    "  aspect['sentiment'] = TextBlob(aspect['description']).sentiment\n",
    "print(aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "##np.save\n",
    "##np.load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazing amazing ADJ JJ amod xxxx True False\n",
      "pic pic NOUN NN nsubj xxx True False\n",
      "n n CCONJ CC cc x True False\n",
      "video video NOUN NN conj xxxx True False\n",
      "... ... PUNCT : punct ... False False\n",
      "love love NOUN NN compound xxxx True False\n",
      "phone phone NOUN NN ROOT xxxx True False\n",
      ".. .. PUNCT NFP punct .. False False\n",
      "nice nice ADJ JJ amod xxxx True False\n",
      "n n CCONJ CC cc x True False\n",
      "fast fast ADJ JJ conj xxxx True False\n",
      ".. .. PUNCT NFP punct .. False False\n",
      "awesome awesome ADJ JJ amod xxxx True False\n",
      "pic pic NOUN NN nmod xxx True False\n",
      "great great ADJ JJ amod xxxx True False\n",
      "battery battery NOUN NN compound xxxx True False\n",
      "life life NOUN NN appos xxxx True False\n",
      ".. .. PUNCT NFP punct .. False False\n",
      "real real ADJ JJ amod xxxx True False\n",
      "flagship flagship NOUN NN compound xxxx True False\n",
      "phone phone NOUN NN appos xxxx True False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text , token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['battery','ram','display','camera','storage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazing pic n video...love phone .. nice n fast .. awesome pic great battery life .. real flagship phone\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.5666666666666668, subjectivity=0.7583333333333333)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(blob)\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"amazing pick n video...love phone .. nice n fast .. awesome pick great battery life .. real flagship phone\")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('preprocessed_array_sentence',sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = np.load('preprocessed_array_sentence.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 0-dimensional, but 1 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-a635d1333a53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreviews\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 0-dimensional, but 1 were indexed"
     ]
    }
   ],
   "source": [
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
